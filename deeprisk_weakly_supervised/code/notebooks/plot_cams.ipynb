{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa17ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# external packages\n",
    "import os\n",
    "import sys\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# own code\n",
    "sys.path.append('../')\n",
    "from make_and_evaluate_cams_2D import evaluate_batch\n",
    "from train_classifier_2d import init_model, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f410f",
   "metadata": {},
   "source": [
    "This notebook serves to plot examples of Class Activation Maps.\n",
    "\n",
    "This notebook might need some updates to function with changes in the rest of the code. Either way, Class Activation Map examples are also saved during training, and are accessible in tensorboard. Alternatively, they can also be viewed using make_and_evaluate_cam(_3D).py with the --visualize flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d1bbc",
   "metadata": {},
   "source": [
    "# Deeprisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b4d885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hparams=Namespace(dataset='deeprisk', trainseed=42, splitseed=84, train_frac=0.6, data_path='\\\\\\\\amc.intra\\\\users\\\\R\\\\rcklein\\\\home\\\\deeprisk\\\\weakly_supervised\\\\data', img_path='all_niftis_n=657', weak_labels_path='weak_labels_n=657.xlsx', myoseg_path='myocard_predictions/version_19', seg_labels_dir='fibrosis_labels_n=75', image_norm='global_statistic', no_roi_crop=False, include_no_myo=False, roi_crop='fixed', center_crop=224, input_size=224, rotate=0, translate=(0, 0), scale=(1, 1), shear=(0, 0, 0, 0), brightness=0, contrast=0, hflip=0.0, vflip=0.0, randomaffine_prob=0, randomcrop=False, randomcrop_prob=0.0, randomerasing_probs=[])\n",
      "train transforms:\n",
      "Compose(\n",
      "    ApplyAllResize(size=(224, 224), interpolations=['bilinear', 'bilinear', 'bilinear'], max_size=None, antialias=None)\n",
      "    ApplyFirstColorJitter(brightness=None, contrast=None, saturation=None, hue=None)\n",
      "    ApplyFirstNormalize(mean=[0.57], std=[0.06])\n",
      "    RandomApply(\n",
      "    p=0\n",
      "    ApplyAllRandomAffine(degrees=[0.0, 0.0], scale=(1, 1), shear=[0.0, 0.0, 0.0, 0.0])\n",
      ")\n",
      ")\n",
      "val_transforms:\n",
      "Compose(\n",
      "    ApplyAllResize(size=(224, 224), interpolations=['bilinear', 'bilinear', 'bilinear'], max_size=None, antialias=None)\n",
      "    ApplyFirstNormalize(mean=[0.57], std=[0.06])\n",
      ")\n",
      "len(PIXEL_LABEL_IDS)=75\n",
      "Train patients: 615\n",
      "len(label_df)=431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "431it [00:36, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(PIXEL_LABEL_IDS)=75\n",
      "Validation patients: 21\n",
      "VAL_IDS=['DRAUMC1094', 'DRAUMC0532', 'DRAUMC1164', 'DRAUMC0235', 'DRAUMC0804', 'DRAUMC0072', 'DRAUMC0847', 'DRAUMC1082', 'DRAUMC0583', 'DRAUMC0481', 'DRAUMC0768', 'DRAUMC1224', 'DRAUMC0923', 'DRAUMC0667', 'DRAUMC1037', 'DRAUMC0431', 'DRAUMC0868', 'DRAUMC0527', 'DRAUMC1122', 'DRAUMC1084', 'DRAUMC0743']\n",
      "len(label_df)=18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:01, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(PIXEL_LABEL_IDS)=75\n",
      "Test patients: 21\n",
      "TEST_IDS=['DRAUMC0075', 'DRAUMC0172', 'DRAUMC0338', 'DRAUMC0380', 'DRAUMC0411', 'DRAUMC0435', 'DRAUMC0507', 'DRAUMC0567', 'DRAUMC0634', 'DRAUMC0642', 'DRAUMC0673', 'DRAUMC1017', 'DRAUMC1042', 'DRAUMC1166', 'DRAUMC1199', 'DRAUMC0051', 'DRAUMC0805', 'DRAUMC0891', 'DRAUMC1049', 'DRAUMC0008', 'DRAUMC0949']\n",
      "len(label_df)=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 4191, positive: 2347\n",
      "Validation data: 173, positive: 76\n",
      "Test data: 198, positive: 129\n",
      "Train data: 4191, validation data 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = Namespace()\n",
    "# choose dataset\n",
    "hparams.dataset = 'deeprisk'\n",
    "# reproducability\n",
    "hparams.trainseed = 42\n",
    "hparams.splitseed = 84\n",
    "hparams.train_frac = 0.6\n",
    "# paths\n",
    "hparams.data_path = r\"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\data\"\n",
    "hparams.img_path = r\"all_niftis_n=657\"\n",
    "hparams.weak_labels_path = r\"weak_labels_n=657.xlsx\"\n",
    "hparams.myoseg_path = r\"myocard_predictions/version_19\"\n",
    "hparams.seg_labels_dir = r\"fibrosis_labels_n=75\"\n",
    "# data augmentation\n",
    "hparams.image_norm = \"global_statistic\"\n",
    "hparams.no_roi_crop = False\n",
    "hparams.include_no_myo = False\n",
    "hparams.roi_crop = \"fixed\" # \"fitted\", or \"fixed\"\n",
    "hparams.center_crop = 224\n",
    "hparams.input_size = 224\n",
    "hparams.rotate = 0\n",
    "hparams.translate = (0, 0)\n",
    "hparams.scale = (1, 1)\n",
    "hparams.shear = (0, 0, 0, 0)\n",
    "hparams.brightness = 0\n",
    "hparams.contrast = 0\n",
    "hparams.hflip = 0.0\n",
    "hparams.vflip = 0.0\n",
    "hparams.randomaffine_prob=0\n",
    "hparams.randomcrop=False\n",
    "hparams.randomcrop_prob = 0.0\n",
    "hparams.randomerasing_probs = []\n",
    "print(f'{hparams=}')\n",
    "\n",
    "pl.seed_everything(hparams.trainseed, workers=True)\n",
    "# prepare dataloaders\n",
    "dataset_train, dataset_val, dataset_test = load_dataset(hparams)\n",
    "print(f\"Train data: {len(dataset_train)}, validation data {len(dataset_val)}\")\n",
    "\n",
    "train_loader = DataLoader(dataset_train,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=False,\n",
    "                            num_workers=1)\n",
    "val_loader = DataLoader(dataset_val,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                        num_workers=1)\n",
    "\n",
    "test_loader = DataLoader(dataset_test,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                        num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7e088",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1e6bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConstructorError",
     "evalue": "could not determine a constructor for the tag 'tag:yaml.org,2002:python/tuple'\n  in \"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\tb_logs\\drnd_masked_avg_dilation=7\\version_0\\hparams.yaml\", line 14, column 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 24\u001b[0m drnd_d_7 \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mamc.intra\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43musers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mR\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mrcklein\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mhome\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdeeprisk\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mweakly_supervised\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtb_logs\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdrnd_masked_avg_dilation=7\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mversion_0\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mepoch=119-step=31320.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m drnd_d_3 \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mamc.intra\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrcklein\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdeeprisk\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mweakly_supervised\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtb_logs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdrnd_masked_avg_dilation=3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mversion_0\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mepoch=119-step=31320.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m drnd_d_3_new \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mamc.intra\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124musers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrcklein\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdeeprisk\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mweakly_supervised\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtb_logs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdrnd_masked_avg_dilation=3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mversion_0\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mepoch=119-step=31320.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(checkpoint_path, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# add hparams from saved model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(MODEL_DIR\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m---> 15\u001b[0m     parsed_yaml \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m parsed_yaml\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(hparams, k, v)\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[1;34m(stream, Loader)\u001b[0m\n\u001b[0;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:51\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_single_node()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:60\u001b[0m, in \u001b[0;36mBaseConstructor.construct_document\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generators \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generator \u001b[38;5;129;01min\u001b[39;00m state_generators:\n\u001b[1;32m---> 60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dummy \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[0;32m     61\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstructed_objects \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:413\u001b[0m, in \u001b[0;36mSafeConstructor.construct_yaml_map\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    411\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m data\n\u001b[1;32m--> 413\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m data\u001b[38;5;241m.\u001b[39mupdate(value)\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:218\u001b[0m, in \u001b[0;36mSafeConstructor.construct_mapping\u001b[1;34m(self, node, deep)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MappingNode):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_mapping(node)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:143\u001b[0m, in \u001b[0;36mBaseConstructor.construct_mapping\u001b[1;34m(self, node, deep)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mHashable):\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile constructing a mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mstart_mark,\n\u001b[0;32m    142\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound unhashable key\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_node\u001b[38;5;241m.\u001b[39mstart_mark)\n\u001b[1;32m--> 143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     mapping[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:100\u001b[0m, in \u001b[0;36mBaseConstructor.construct_object\u001b[1;34m(self, node, deep)\u001b[0m\n\u001b[0;32m     98\u001b[0m             constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_mapping\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag_suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 100\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     data \u001b[38;5;241m=\u001b[39m constructor(\u001b[38;5;28mself\u001b[39m, tag_suffix, node)\n",
      "File \u001b[1;32mH:\\Downloads\\Miniconda3_alt\\envs\\deeprisk_weak\\lib\\site-packages\\yaml\\constructor.py:427\u001b[0m, in \u001b[0;36mSafeConstructor.construct_undefined\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_undefined\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[1;32m--> 427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    428\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not determine a constructor for the tag \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m node\u001b[38;5;241m.\u001b[39mtag,\n\u001b[0;32m    429\u001b[0m             node\u001b[38;5;241m.\u001b[39mstart_mark)\n",
      "\u001b[1;31mConstructorError\u001b[0m: could not determine a constructor for the tag 'tag:yaml.org,2002:python/tuple'\n  in \"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\tb_logs\\drnd_masked_avg_dilation=7\\version_0\\hparams.yaml\", line 14, column 7"
     ]
    }
   ],
   "source": [
    "# load a trained model\n",
    "#resnet18 = LightningResNet(no_cam=False, highres=True, model='resnet18', num_classes=1, in_chans=1)\n",
    "#resnet18.load_state_dict(torch.load(r\"..\\..\\tb_logs\\resnet18\\version_9\\checkpoints\\epoch=249-step=37749.ckpt\", map_location=torch.device('cpu'))['state_dict'])\n",
    "#resnet18.eval()\n",
    "def load_model(checkpoint_path, model=\"drnd\"):\n",
    "    # path names\n",
    "    MODEL_DIR = Path(checkpoint_path).parent.parent\n",
    "    setattr(hparams, \"model\", model)\n",
    "    setattr(hparams, \"singlestage\", False)\n",
    "    setattr(hparams, \"load_checkpoint\", checkpoint_path)\n",
    "    \n",
    "    \n",
    "    # add hparams from saved model\n",
    "    with open(MODEL_DIR.joinpath(\"hparams.yaml\"), 'r') as stream:\n",
    "        parsed_yaml = yaml.safe_load(stream)\n",
    "        for k, v in parsed_yaml.items():\n",
    "            setattr(hparams, k, v)\n",
    "\n",
    "    # load model\n",
    "    model = init_model(hparams)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "drnd_d_7 = load_model(r\"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\tb_logs\\drnd_masked_avg_dilation=7\\version_0\\checkpoints\\epoch=119-step=31320.ckpt\")\n",
    "drnd_d_3 = load_model(r\"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\tb_logs\\drnd_masked_avg_dilation=3\\version_0\\checkpoints\\epoch=119-step=31320.ckpt\")\n",
    "drnd_d_3_new = load_model(r\"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\tb_logs\\drnd_masked_avg_dilation=3\\version_0\\checkpoints\\epoch=119-step=31320.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "from skimage.filters import threshold_multiotsu\n",
    "def plot_cams_gen(dataloader, net, wrong_only=False):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[\"img\"]\n",
    "        labels = batch[\"label\"]\n",
    "        myo_seg = batch[\"myo_seg\"]\n",
    "        batch_path = batch[\"img_path\"]\n",
    "        batch_slice = batch[\"slice_idx\"]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs, myo_seg=myo_seg)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5)[:, 0]\n",
    "        \n",
    "        cam = net.make_cam(inputs, myo_seg=myo_seg, select_values=\"pos_only\", upsampling=\"conv_transpose\").detach()\n",
    "        #cam = torchvision.transforms.Resize(inputs.shape[2], interpolation=transforms.InterpolationMode.NEAREST)(cam)\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            if labels[i].item() != int(preds[i].item()) or wrong_only == False:\n",
    "                # create the histogram\n",
    "                histogram, bin_edges = np.histogram(cam[i][0], bins=20)\n",
    "                print(f'{histogram=}')\n",
    "                # configure and draw the histogram figure\n",
    "                plt.figure()\n",
    "                plt.title(\"CAM Histogram\")\n",
    "                plt.xlabel(\"Value\")\n",
    "                plt.ylabel(\"pixel count\")\n",
    "                plt.xlim([0.0, 1.0])  # <- named arguments do not work here\n",
    "\n",
    "                plt.plot(bin_edges[0:-1], histogram)  # <- or here\n",
    "                plt.show()\n",
    "                \n",
    "                \n",
    "                print(\"label: \", labels[i].item(), \", predicted: \", int(preds[i].item()), ', score:', torch.sigmoid(outputs[i]).detach().numpy())\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                print(f\"slice {batch_slice[i]}, file {batch_path[i]}\")\n",
    "                print(f\"{cam[i].max()}\")\n",
    "                axs[0].imshow(inputs[i][0], cmap=\"gray\")\n",
    "                axs[1].imshow(inputs[i][0], cmap='gray')\n",
    "                axs[1].imshow(cam[i][0], cmap=\"coolwarm\", alpha=0.5, vmin=0.0, vmax=1)\n",
    "                fig.colorbar(cm.ScalarMappable(cmap=\"coolwarm\"))\n",
    "                plt.show()\n",
    "                yield\n",
    "        \n",
    "from make_and_evaluate_cams_2D import evaluate_batch\n",
    "\n",
    "\n",
    "\n",
    "def plot_cams_crf_gen(dataloader, net, crf_params, iters, threshold, otsu_cam_threshold=0.0):\n",
    "    for batch in dataloader:\n",
    "        if batch[\"fibrosis_seg_label\"].amax() == 1:\n",
    "            print(f\"{batch['img_path']} {batch['slice_idx']}\")\n",
    "            image = batch[\"img\"].detach().numpy()[0][0]\n",
    "            gt = batch[\"fibrosis_seg_label\"].numpy()[0][0]\n",
    "            \n",
    "            cam = net.make_cam(batch[\"img\"], myo_seg=batch[\"myo_seg\"], upsampling=\"conv_transpose\", select_values=\"pos_only\")\n",
    "            cam = cam[0][0].detach().numpy()\n",
    "            hist, bin_centers = skimage.exposure.histogram(image[cam > otsu_cam_threshold], nbins=256)\n",
    "            thresholds = threshold_multiotsu(hist=(hist, bin_centers), classes=3)\n",
    "            # Using the threshold values, we generate the three regions.\n",
    "            regions = np.digitize(image, bins=thresholds)\n",
    "            regions[cam <= otsu_cam_threshold] = 3\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 3.5))\n",
    "\n",
    "            # Plotting the original image.\n",
    "            ax[0].imshow(image, cmap='gray')\n",
    "            ax[0].set_title('Original')\n",
    "            ax[0].axis('off')\n",
    "\n",
    "            # Plotting the histogram and the two thresholds obtained from\n",
    "            # multi-Otsu.\n",
    "            ax[1].hist(image[cam > otsu_cam_threshold].ravel(), bins=255)\n",
    "            ax[1].set_title('Histogram')\n",
    "            for thresh in thresholds:\n",
    "                ax[1].axvline(thresh, color='r')\n",
    "\n",
    "            # Plotting the Multi Otsu result.\n",
    "            ax[2].imshow(regions, cmap='jet')\n",
    "            ax[2].set_title('Multi-Otsu result')\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            ax[3].hist(image[gt > 0.5].ravel(), bins=255)\n",
    "            ax[3].set_title('ground truth fibrosis histogram')\n",
    "            \n",
    "\n",
    "            plt.subplots_adjust()\n",
    "\n",
    "            plt.show()\n",
    "            metrics, pseudo_metrics, pseudo = evaluate_batch(batch, net, confidence_weighting=True,\n",
    "                                                               threshold=threshold, iters=iters, crf_params=crf_params,\n",
    "                                                               visualize=True, otsu_cam_threshold=0.0, otsu_mask=True,\n",
    "                                                               compute_no_label=False, merge_cam=True)\n",
    "            yield    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebc5d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selected_model = drnd_d_3\n",
    "w1, alpha, beta, w2, gamma, iters = 10, 3, 3, 1, 3, 5\n",
    "crf_params = w1, alpha, beta, w2, gamma, iters\n",
    "threshold = 0.3\n",
    "\n",
    "train_cam_examples = plot_cams_crf_gen(train_loader, selected_model, crf_params, iters, threshold)\n",
    "val_cam_examples = plot_cams_crf_gen(val_loader, selected_model, crf_params, iters, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5002bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(train_cam_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0658c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(val_cam_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11d588",
   "metadata": {},
   "source": [
    "# Make nice plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_and_evaluate_cams_2D import dice_score, cam_to_prob, otsu_mask_cam\n",
    "from preprocessing import denormalize_transform\n",
    "from densecrf import densecrf\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "\n",
    "def plot_grid(dataloader, model, crf_params, iters, threshold, n_imgs=16):\n",
    "    DENORM = denormalize_transform()\n",
    "    # collect images\n",
    "    collected_imgs, collected_gt, collected_pseudo = [], [], []\n",
    "    for batch in dataloader:        \n",
    "        img = batch[\"img\"]\n",
    "        weak_label = batch[\"label\"]\n",
    "        myo_seg = batch['myo_seg']\n",
    "        fibrosis_seg_label = batch[\"fibrosis_seg_label\"]\n",
    "        # compute dice if label available\n",
    "        if fibrosis_seg_label.amax() == 1:\n",
    "            cam = model.make_cam(img, select_values=\"pos_only\",\n",
    "                                    upsampling=\"conv_transpose\",\n",
    "                                    myo_seg=myo_seg).detach()\n",
    "            pred = model.output2pred(model(img, myo_seg=myo_seg))\n",
    "            \n",
    "            cam = cam * pred\n",
    "\n",
    "            pseudo = otsu_mask_cam(cam, img, 0)\n",
    "            \n",
    "            if iters > 0:\n",
    "                # denseCRF\n",
    "                denorm_img = DENORM(img)\n",
    "                img_int8 = (255*img).type(torch.uint8)\n",
    "                probs = cam_to_prob(pseudo, cam_threshold=threshold, binarize=True)\n",
    "                pseudo = densecrf(img_int8[0], probs[0], crf_params)\n",
    "                pseudo_dice = dice_score(pseudo, fibrosis_seg_label, 0.5)\n",
    "            else:\n",
    "                pseudo_dice = None\n",
    "                \n",
    "            collected_imgs.append(img)\n",
    "            collected_gt.append((fibrosis_seg_label > 0.0))\n",
    "            collected_pseudo.append(pseudo)\n",
    "            \n",
    "            if len(collected_imgs) >= n_imgs:\n",
    "                # start plotting\n",
    "                nrows = int(np.sqrt(n_imgs))\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(14, 5))\n",
    "                fig.suptitle(\"Weakly supervised fibrosis segmentation\")\n",
    "\n",
    "\n",
    "                imgs = torch.cat(collected_imgs, dim=0)\n",
    "                grid_img = torchvision.utils.make_grid(imgs, nrow=nrows, normalize=True, padding=2).detach().cpu()\n",
    "                axs[0].imshow(grid_img[0,:,:], cmap=\"gray\")\n",
    "                axs[0].set_title(\"Reference image\")\n",
    "\n",
    "                gt = torch.cat(collected_gt, dim=0)\n",
    "                print(gt.shape)\n",
    "                grid_gt = torchvision.utils.make_grid(gt, nrow=nrows, padding=2).cpu().detach()\n",
    "                grid_gt = np.ma.masked_where(grid_gt <= 0, grid_gt)\n",
    "                axs[1].imshow(grid_img[0,:,:], cmap=\"gray\")\n",
    "                axs[1].imshow(grid_gt[0,:,:], cmap=\"Reds\", alpha=0.5, vmin=0, vmax=1)\n",
    "                axs[1].set_title(\"Ground truth\")\n",
    "\n",
    "\n",
    "                pseudo = torch.cat(collected_pseudo, dim=0)\n",
    "                print(pseudo.shape)\n",
    "                grid_pseudo = torchvision.utils.make_grid(pseudo, nrow=nrows, padding=2).cpu().detach()\n",
    "                grid_pseudo = np.ma.masked_where(grid_pseudo <= 0, grid_pseudo)\n",
    "                axs[2].imshow(grid_img[0,:,:], cmap=\"gray\")\n",
    "                axs[2].imshow(grid_pseudo[0,:,:], cmap=\"Reds\", alpha=0.5, vmin=0, vmax=1)\n",
    "                axs[2].set_title(\"Pseudo label\")\n",
    "\n",
    "                axs[0].set_axis_off()\n",
    "                axs[1].set_axis_off()\n",
    "                axs[2].set_axis_off()\n",
    "\n",
    "                # reset containers\n",
    "                collected_imgs, collected_gt, collected_pseudo = [], [], []\n",
    "                yield\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "pseudo_grid_gen = plot_grid(test_loader, selected_model, crf_params, iters, threshold, n_imgs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e8c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(pseudo_grid_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torchvision.transforms import transforms\n",
    "import cv2\n",
    "irange = range\n",
    "\n",
    "def plot_cams_grid_gen(dataloader, net, wrong_only=False):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[-2]\n",
    "        labels = batch[-1]\n",
    "        batch_paths = batch[0]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        #_, preds = torch.max(outputs, 1)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5)[:, 0].int()\n",
    "        \n",
    "        cam = net.make_cam(inputs, select_values=\"sigmoid\", upsampling=\"conv_transpose\").detach()\n",
    "        print(cam.shape, cam.max(), cam.min())\n",
    "        #cam = torchvision.transforms.Resize(inputs.shape[2], interpolation=transforms.InterpolationMode.NEAREST)(cam)\n",
    "        \n",
    "        \n",
    "        labeled_grid = make_grid_with_labels(inputs, labels, preds, nrow=4, normalize=True)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(labeled_grid.permute(1, 2, 0)[:,:,0], cmap=\"gray\")\n",
    "        plt.show()\n",
    "        yield\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def make_grid_with_labels(tensor, labels, predictions, nrow=8, limit=20, padding=2,\n",
    "                          normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Make a grid of images.\n",
    "\n",
    "    Args:\n",
    "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "            or a list of images all of the same size.\n",
    "        labels (list):  ( [labels_1,labels_2,labels_3,...labels_n]) where labels is Bx1 vector of some labels\n",
    "        limit ( int, optional): Limits number of images and labels to make grid of\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
    "            The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\n",
    "        padding (int, optional): amount of padding. Default: ``2``.\n",
    "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
    "            by the min and max values specified by :attr:`range`. Default: ``False``.\n",
    "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
    "            then these numbers are used to normalize the image. By default, min and max\n",
    "            are computed from the tensor.\n",
    "        scale_each (bool, optional): If ``True``, scale each image in the batch of\n",
    "            images separately rather than the (min, max) over all images. Default: ``False``.\n",
    "        pad_value (float, optional): Value for the padded pixels. Default: ``0``.\n",
    "\n",
    "    Example:\n",
    "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
    "\n",
    "    \"\"\"\n",
    "    # Opencv configs\n",
    "    if limit is not None:\n",
    "        tensor = tensor[:limit, ::]\n",
    "        labels = labels[:limit]\n",
    "        predictions = predictions[:limit]\n",
    "\n",
    "    \n",
    "    font = 1\n",
    "    fontScale = 1\n",
    "    color = (255, 0, 0)\n",
    "    thickness = 1\n",
    "\n",
    "    if not (torch.is_tensor(tensor) or\n",
    "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
    "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
    "\n",
    "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor, dim=0)\n",
    "\n",
    "    if tensor.dim() == 2:  # single image H x W\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    if tensor.dim() == 3:  # single image\n",
    "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
    "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
    "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
    "\n",
    "    if normalize is True:\n",
    "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "        if range is not None:\n",
    "            assert isinstance(range, tuple), \\\n",
    "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
    "\n",
    "        def norm_ip(img, min, max):\n",
    "            img.clamp_(min=min, max=max)\n",
    "            img.add_(-min).div_(max - min + 1e-5)\n",
    "\n",
    "        def norm_range(t, range):\n",
    "            if range is not None:\n",
    "                norm_ip(t, range[0], range[1])\n",
    "            else:\n",
    "                norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "        if scale_each is True:\n",
    "            for t in tensor:  # loop over mini-batch dimension\n",
    "                norm_range(t, range)\n",
    "        else:\n",
    "            norm_range(tensor, range)\n",
    "\n",
    "    if tensor.size(0) == 1:\n",
    "        return tensor.squeeze(0)\n",
    "\n",
    "    # make the mini-batch of images into a grid\n",
    "    nmaps = tensor.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
    "    num_channels = tensor.size(1)\n",
    "    grid = tensor.new_full((num_channels, height * ymaps + padding, width * xmaps + padding), pad_value)\n",
    "    k = 0\n",
    "    for y in irange(ymaps):\n",
    "        for x in irange(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            working_tensor = tensor[k]\n",
    "            if labels is not None:\n",
    "                org = (0, int(tensor[k].shape[1] * 0.1))\n",
    "                working_image = cv2.UMat(\n",
    "                    np.asarray(np.transpose(working_tensor.numpy(), (1, 2, 0)) * 255).astype('uint8'))\n",
    "                image = cv2.putText(working_image, f\"label: {str(labels[k].item())}      pred: {str(predictions[k].item())}\",\n",
    "                                    org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "                working_tensor = transforms.ToTensor()(image.get())\n",
    "            grid.narrow(1, y * height + padding, height - padding) \\\n",
    "                .narrow(2, x * width + padding, width - padding) \\\n",
    "                .copy_(working_tensor)\n",
    "            k = k + 1\n",
    "    return grid\n",
    "\n",
    "selected_model = drnd24_maxpool\n",
    "grid_examples = plot_cams_grid_gen(train_loader, selected_model, wrong_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bb929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(grid_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4863c22",
   "metadata": {},
   "source": [
    "# CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ColorJitter(brightness=0.0, contrast=0.0),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.RandomAffine(90, scale=(0.8, 1.2), shear=(-5, 5, -5, 5), translate=(0, 0))    \n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "print(train_transforms)\n",
    "print(val_transforms)\n",
    "\n",
    "\n",
    "# path names\n",
    "DATA_DIR = Path(r\"\\\\amc.intra\\users\\R\\rcklein\\home\\deeprisk\\weakly_supervised\\data\")\n",
    "CIFAR_DIR = DATA_DIR.joinpath('cifar10_data')            \n",
    "assert DATA_DIR.exists()\n",
    "\n",
    "cifar_train = torchvision.datasets.CIFAR10(root=CIFAR_DIR, train=True,\n",
    "                                download=True, transform=train_transforms)\n",
    "\n",
    "cifar_val = torchvision.datasets.CIFAR10(root=CIFAR_DIR, train=False,\n",
    "                                    download=True, transform=val_transforms)\n",
    "\n",
    "\n",
    "cifar_train_dataloader = DataLoader(cifar_train, batch_size=8, shuffle=False)\n",
    "cifar_val_dataloader = DataLoader(cifar_val, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cams_gen_cifar(dataloader, net, wrong_only=False):\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[-2]\n",
    "        labels = batch[-1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        #preds = (torch.sigmoid(outputs) > 0.5)[:, 0]\n",
    "        \n",
    "        cam = net.make_cam(inputs).detach()\n",
    "        print(cam.shape)\n",
    "        cam = torchvision.transforms.Resize(inputs.shape[2], interpolation=transforms.InterpolationMode.NEAREST)(cam)\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            if labels[i].item() != int(preds[i].item()) or wrong_only == False:\n",
    "                print(\"label: \", f\"{classes[labels[i].item()]} ({labels[i].item()})\", \", predicted: \", f\"{classes[int(preds[i].item())]} ({int(preds[i].item())})\", ', scores:', F.softmax(outputs[i], dim=0).detach().numpy())\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "                # get back image\n",
    "                img = (inputs[i].permute(1,2,0)) / 2 + 0.5\n",
    "                axs[0].imshow(img)\n",
    "                axs[1].imshow(img)\n",
    "                axs[1].imshow(cam[i][labels[i].item()], cmap=\"jet\", alpha=0.5)\n",
    "                plt.show()\n",
    "                yield\n",
    "selected_model = resnet18_cifar\n",
    "train_cam_examples_cifar = plot_cams_gen_cifar(cifar_train_dataloader, selected_model, wrong_only=False)\n",
    "val_cam_examples_cifar = plot_cams_gen_cifar(cifar_val_dataloader, selected_model, wrong_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b848811",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_cam_examples_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(val_cam_examples_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3005b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet18_7x7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5dceb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(convnext_narrow_shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7441c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
